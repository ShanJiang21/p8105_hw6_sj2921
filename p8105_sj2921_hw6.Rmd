---
title: "p8105_hw6_sj2921"
author: "Shan Jiang"
date: "11/16/2018"
output: github_document
---
## problem 1
```{r}
library(tidyverse)
library(broom)
library(Hmisc)
library(modelr)
library(mgcv)
library(ggplot2)
```

```{r}
### Import the raw data.

homicide_raw = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

### 1.1 Tidy the dataset

```{r}
homicide_df = homicide_raw %>% 
  mutate(city_state = str_c(city, state, sep = "," )) %>% 
  mutate(case_status = as.numeric(disposition == "Closed by arrest")) %>% 
  filter(!city_state %in% c("Dallas,TX", "Phoenix,AZ","Kansas City,MO","Tulsa,AL" )) %>%
   ### relevel `victim_race`
  mutate(victim_race = 
           fct_relevel(ifelse(victim_race == "White", "white", "non-white"), "white")) %>% 
  ## change the victim_age as numeric
  mutate(victim_age = as.numeric(victim_age)) 
  
```

Since there are three levels for the factor of disposition, we need to recode it as whether the homicide is solved or not: for *Closed by arrest*, we coined it as 0 while  adding 1 for "*Open/No arrest*" or "*closed without arrest*".

For categories white and non-white, with white as the reference category. 


### 1.2 Simulation

(1)  Baltimore, MD Models 
For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, race(as just classified) and sex as predictors. 

```{r}
Bal_logit = 
  homicide_df %>% 
  filter(city_state == "Baltimore,MD") %>% 
  glm(case_status ~ victim_age  + victim_sex + victim_race , data = ., family = binomial) 

## Obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing non-white victims to white victims keeping all other variables fixed.
Bal_logit %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%  ## sig.level = 0.05, critical value = 1.96
  mutate(CI.lower =  exp(estimate - std.error * 1.96)) %>% 
  mutate(CI.higher =  exp(estimate + std.error * 1.96)) %>%
  select(term, log_OR = estimate, OR, CI.lower, CI.higher, p.value) %>% 
  knitr::kable(digits = 3)
```

*Comment*: the estimator of odds ratio is 0.441 < 1 (95% CI: [0.313, 0.620]), implying that in Baltimore city, the odds of being murdered is 0.441 times lower among non-white citizens than white. Because the odds ratio is under 1, which means being non-white can exert protective effect for avoiding being murdered.  


## Each city:Compare the white and non-white values 

Do this within a “tidy” pipeline, making use of `purrr::map`, `list columns`, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

```{r}
## Create a list column for the city_state dataset

homicide_nest = homicide_df %>% 
  group_by(city_state) %>% 
  nest(victim_race:case_status)

head(homicide_nest)

## Create a glm function 
homicide_glm  = function(df) {
  glm = glm(case_status ~ victim_age + victim_race + victim_sex, data = df, family = binomial()) %>% 
  broom::tidy() 
  
  glm 
}


## Apply to each city, state 

city_murder =  homicide_nest %>% 
    mutate(models = map(homicide_nest$data, homicide_glm )) %>% 
    select(-data) %>% 
    unnest() 

## Add CI, city and tidy 
city_murder = city_murder %>%  
        mutate(OR = exp(estimate),
               log_OR = estimate) %>%
        filter(term == "victim_racenon-white") %>% 
        mutate(CI.low =  exp(estimate - std.error * 1.96) ) %>% 
        mutate(CI.high =  exp(estimate + std.error * 1.96)) %>% 
        select(city_state, term, log_OR, OR, p.value, CI.low, CI.high) %>% 
        mutate(city_state = fct_reorder(city_state, OR)) 
  
```

## Plot 

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r out.width = "70%", fig.height = 7, fig.align = "center"}
city_murder %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point(alpha = 0.52) +
  geom_errorbar(mapping = aes(ymin = CI.low, ymax = CI.high, colour = "darkred" )) +
  theme_bw() +
  theme(legend.position = "none",
        legend.direction = "horizontal",
         legend.key.size = unit(0.06, "cm")) + 
  coord_flip() + 
      labs(x = "City State",
           y = "City Homicide Odds ratio", 
           title = "Homicide Odds ratio of race white vs. Non-white by City, state", 
           subtitle = "Error Bar Using mean as center with Confidence Intervals", 
           caption = "source: Washington Post") 

```


## Problem 2

### Importation and cleaning the data 
```{r}
birth_weight = read_csv("./Data/birthweight.csv") 
  

birth_weight = birth_weight %>% 
    janitor::clean_names() %>% 
    mutate(babysex = as.factor(recode(babysex, `1` = 0, `2` = 1)),
           malform = as.factor(malform),
           mrace = as.factor(mrace),
           frace = as.factor(frace)) %>% 
    mutate(bhead = as.numeric(bhead), 
            bwt = as.numeric(bwt * 0.00220462),
            mheight = as.numeric(mheight ),
            mheight = as.numeric(mheight )) 

# no missing data
skimr::skim(birth_weight )

```

* There are `nrow(birth_weight)` observations and `ncol(birth_weight)` variables in the dataset. There are 4 factors as babysex, presence of malformations, mother and father's race. Remaining variables are of numeric format.

* We need to transform the Unit of the `bwt` into pounds because it is consistent with other variables measured in pounds.

* We recoded the sex of baby for the convenience of analyzing.



2. Model procedure: 

(1) Exploration of correlation and distribution.

```{r}
## Distribution of outcome variable
library("gridExtra")
his = birth_weight %>% 
  ggplot(aes(x = bwt, y = ..density..)) + 
  geom_histogram() +
  geom_density(color = "red") +
  ggtitle("Histogram of birthweight variable")

box = birth_weight %>% 
  ggplot(aes(y = bwt)) + 
  labs(y = "birthweight") +
  geom_boxplot() +
  ggtitle("Boxplot of birthweight variable")

grid.arrange(his, box, nrow = 1, top = "Distribution of outcome variable-Birthweight") 

```

The distribution of `birthweight` is quite normal, which satisfies the linear regression model assumptions.

```{r}
## Excluding two variables contatining many 0s.
summary(birth_weight$pnumlbw )
summary(birth_weight$pnumsga)
```

* There are 2 variables containing all 0s as their observation value: `pnumlbw ` and `pnumsga` in the birth weight data, we would **exclude** them in our model.

#### I. The **hypothesis only model** is based on variables more relevant to **physical aspects** of mother :

1. Baby's sex(which can be a potential interaction term, we may stratify our analysis by sex);

2. Also, the health status of mother who gave birth to the baby is also important, this model contains the `race` of mother, `gestational age` in weeks, mother’s `weight gain`, `height` and so on. The most obvious one is mother’s age at delivery (a strong negative correlation in literature), the birth history, measured by `parity`, can provide important context for understanding mom's health, the average number of cigarettes smoked during the pregnancy, which is a significant exposure for affecting child's health is also included in the model.  

3. Meanwhile, I chose family monthly income as an indicator of SES of the family, which can be informative for important social factor for analysis.

4. Correlation and Collinearity: The `mheigth`(mother’s height) and `ppbmi`(mother’s pre-pregnancy BMI) is clearly correlated, also we can see that the `ppwt`, `delwt` and `wtgain` are highly collinearlized because we can simply derive the weight gain `wtgain` from the first two variables, and it implies that the `wegain` is a linear combination of the `ppwt`, `delwt`.  So we have to drop the `wtgain` if we want to include the `ppwt` and `delwt`at the same time. For parsimonous reason, we would only include `wtgain`in our model. 

#### II. For constructing a model precisely correlated with the outcome variable, we simply do a correlation matrix with the birthweight.

```{r}
## Exploration of correlation for numeric variables in dataset.
correlation_df = birth_weight %>%
  select(-babysex, -frace, -mrace, -malform) %>% 
  cor(.[3], .) %>% 
  broom::tidy() %>% 
  select(-.rownames, -bwt) 
## Filtering the variables which have a correlation with bwt as of higher than 0.2.
correlation_df %>% 
  mutate(index = 1) %>% 
  gather(index,bhead:wtgain, factor_key = TRUE) %>%
  mutate( value = `bhead:wtgain`, 
          variable = index ) %>% 
  select(variable, value ) %>% 
  filter(abs(value) > 0.2) %>% 
    knitr::kable(digits = 4) 
```

while the five variables has a moderate correlation with the birthweight of the baby, we still need to include the sex in stratified ananlysis by keeping sex as a control variable.

#### III. Combination of birthweight literature and data-driven model-building 

The last alternative for birthweight may be selected as a combination of these two models. 


### 1.Modeling process

#### I.based on a hypothesized structure for the factors that underly birthweight

Stepwise elimination 

* From the exploration analysis, the baseline of `mrace` and `frace` are 1，denoting all other races are compared to the White. The baseline of sex is 0 as male. 

* The full model has an R-square of 0.7183 and the adjusted R-square is 0.717, while the `menarche` and `malform1 `and `frace8` shows high p-value, we may drop the one with largest_value.

####  (1). Hypothesized model 

```{r}
hyp_mlr = birth_weight %>% 
  lm(bwt ~  babysex  +
     mrace  + fincome + gaweeks + mheight  + 
     parity + smoken + wtgain, data = . )

summary(hyp_mlr)
```

* The Hypothesized model has an R square of 0.3125 and a 0.3109 for Adjusted R-squared, which decreases little from the full model proposed above, meaning that the explanation of variables here is okay.

####  (2). Data_driven model: 

```{r}
Corr_mlr = birth_weight %>% 
  lm(bwt ~  babysex  + 
     bhead + blength + fincome + 
     delwt + gaweeks + wtgain , data = . )

summary(Corr_mlr)
```

* Both the `babies length` and `head circumference` which are most likely to influence the babies' weight, however, it is unlikely to intervene the baby longer or have a larger head.

* The Data-driven model has an R square of 0.702 and an Adjusted R-squared is 0.7016, varies a lot from the hypothesized model proposed above, meaning that the model should include the physical indicators of babies to fit better.


#### (3). Combination of birthweight literature and data-driven model-building 

```{r}
Comb_mlr = birth_weight %>% 
  lm(bwt ~  babysex  + mrace  +
     bhead + blength + fincome + gaweeks + mheight  + 
     parity + smoken + wtgain + delwt , data = . )

summary(Comb_mlr)
```


The combined model owns the highest R square value 0.7181 and a high adjusted-R square value of 0.7173 , higher than the above two models, so we shall test whether it can the best one by cross validation.

### 2.Cross Validation 

```{r message=FALSE, warning=FALSE}
set.seed(1)

cv_df = crossv_mc(birth_weight, 100) 

## construct our train dataset and test dataset 
cv_df =
  cv_df %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

## Mutate Train data sets and test data sets
cv_df <- cv_df %>% 
    mutate(hyp_mlr = map(train, ~lm(bwt ~  babysex + mrace  + fincome + gaweeks + mheight  + 
     parity + smoken + wtgain, data = .x)),
           Corr_mlr = map(train, ~lm(bwt ~ bhead + blength + gaweeks + wtgain + delwt, data = .x)), 
           Comb_mlr = map(train, ~lm(bwt ~ babysex + bhead + blength + 
                    mrace  + fincome + gaweeks + mheight + 
                    parity + smoken + wtgain + delwt , data = .x))) %>% 
    mutate(rmse_hyp = map2_dbl(hyp_mlr, test, ~rmse(model = .x, data = .y)),
           rmse_Corr = map2_dbl(Corr_mlr, test, ~rmse(model = .x, data = .y)),
           rmse_Comb = map2_dbl(Comb_mlr, test, ~rmse(model = .x, data = .y)))
```


### 3. fit a model and plot

I’m mostly focused on RMSE as a way to compare these models, and the plot below shows the distribution of RMSE values for each candidate model.

```{r message=FALSE, warning=FALSE}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  ggtitle("Volin plot of RMSE in each candidate model")
```

Specific value of RMSE 
```{r}
cv_df %>% 
  select( rmse_hyp, rmse_Corr, rmse_Comb) %>% 
  gather(key = model, value = rmse) %>% 
  group_by(model) %>% 
  summarise(mean(rmse)) %>% 
  mutate(rmse = `mean(rmse)`) %>% 
  select(-`mean(rmse)`) %>% 
  knitr::kable(digits = 4)
  
```

From the violin plot, we can see that the hyp model is  worse than the latter 2 models as the RMSE is much higher. As the combined model maintains the lowest RMSE in the model, we may select this one, but it worth more thinking. 

Comparatively speaking, rmse_hyp equals to 0.9367, which is pretty high, so we may not choose the hypothesized model.


### 4. Final model plots 

show a plot of model residuals against fitted values – use `add_predictions` and add_residuals in making this plot.

we choose the combined model in the above, containing all proposed variables in the model.

```{r}
# creating model using combined model

final_mod <- birth_weight %>% 
  lm(bwt ~ babysex + bhead + blength + 
                    mrace  + fincome + gaweeks + mheight + 
                    parity + smoken + wtgain + delwt , data = .) 

birth_weight %>% 
  add_predictions(final_mod) %>% 
  add_residuals(final_mod) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  geom_smooth(se = FALSE) + 
  labs( 
       x = "Predicted value", 
       y = "Residual") +
  ggtitle("Predicted values vs. residuals plot for Final model")

```

From the fitted line and the obs point graph, we can see that the residual is high at the low value,meaning that there are some outliers when pv is low.

### Comparison with other 2 models

```{r}
cv_df2 <- crossv_mc(birth_weight, 100)

cv_df2 <- cv_df2 %>% 
  mutate(Comb_mod = map(train, ~lm(bwt ~ babysex + bhead + blength + 
                    mrace  + fincome + gaweeks + mheight + 
                    parity + smoken + wtgain + delwt , data = .x)), 
         main_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)), 
         other_mod = map(train, ~lm(bwt ~ (bhead * blength * babysex), data = .x))) %>% 
  mutate(rmse_comb = map2_dbl(Comb_mod, test, ~rmse(model = .x, data = .y)),
         rmse_main = map2_dbl(main_mod, test, ~rmse(model = .x, data = .y)),
         rmse_other = map2_dbl(other_mod, test, ~rmse(model = .x, data = .y)))

cv_df2 %>% 
  select( rmse_comb, rmse_main, rmse_other) %>% 
  gather(key = model, value = rmse) %>% 
  ggplot(aes(x = fct_inorder(model), y = rmse)) + 
  geom_violin() + 
  labs(x = "Model", 
       y = "RMSE") +
  ggtitle("Violin plot of RMSE for 3 models")

```

Specific value of RMSE 
```{r}
cv_df2 %>% 
  select( rmse_comb, rmse_main, rmse_other) %>% 
  gather(key = model, value = rmse) %>% 
  group_by(model) %>% 
  summarise(mean(rmse)) %>% 
  mutate(rmse = `mean(rmse)`) %>% 
  select(-`mean(rmse)`) %>% 
  knitr::kable(digits = 4)
  
```

From the Violin plot of RMSE for 3 models, we can see that the comb model has the lowest RMSE value among the three models. For other 2 models, the three way interaction model has a rather lower RMSE thah the main effect model. 

The term RMSE is always between 0 and 1, 0.7 is comparatively high in this case.

```{r}

```

